{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MongoDB Childcare Database\n",
    "This databse is a collection of data about children, staff and their events in a daycare.\n",
    "\n",
    "You will notice the first way we have it setup is very relational. \n",
    "\n",
    "We have a table for `children`, `staff` and `events`. We have a relationship between children, staff and events via the `childId` and the `staffId` on the events table linking back to the `children` and `staff` tables. \n",
    "\n",
    "```mermaid\n",
    "erDiagram\n",
    "    children ||--o{ dailyEvents : has\n",
    "    staff ||--o{ dailyEvents : manages\n",
    "\n",
    "    children {\n",
    "        ObjectId _id\n",
    "        string firstName\n",
    "        string lastName\n",
    "        date dateOfBirth\n",
    "        datetime enrollmentDate\n",
    "        array allergies\n",
    "        object emergencyContact\n",
    "    }\n",
    "\n",
    "    staff {\n",
    "        ObjectId _id\n",
    "        string firstName\n",
    "        string lastName\n",
    "        date hireDate\n",
    "        string role\n",
    "        array credentials\n",
    "    }\n",
    "\n",
    "    dailyEvents {\n",
    "        ObjectId _id\n",
    "        ObjectId childId\n",
    "        ObjectId staffId\n",
    "        datetime timestamp\n",
    "        string eventType\n",
    "        string details\n",
    "        string notes\n",
    "    }\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from pprint import pprint\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "from pymongo.synchronous.command_cursor import CommandCursor\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# check to see if .env file exists and load it\n",
    "if os.path.exists('../.env'):\n",
    "    load_dotenv()\n",
    "else:\n",
    "    raise FileNotFoundError(\".env file not found. Did you copy the .env.example file and rename it to .env?\")\n",
    "\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "DB_PORT = os.getenv('DB_PORT')\n",
    "\n",
    "\n",
    "client = MongoClient(f'mongodb://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}')\n",
    "\n",
    "db = client[DB_NAME]\n",
    "\n",
    "def get_time_for_filters(days_to_go_back: int = 3) -> datetime:\n",
    "\n",
    "    result = db['dailyEvents'].find(\n",
    "        filter={},\n",
    "        sort={'timestamp': -1},\n",
    "        limit=1\n",
    "    )   \n",
    "\n",
    "    ts = result[0]['timestamp']\n",
    "    # get events from 14 days ago(figure 2 weeks worth of events)\n",
    "    ts = result[0]['timestamp'] - timedelta(days=days_to_go_back)\n",
    "    # Go to the start of the day\n",
    "    return ts.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "\n",
    "events_since_time = get_time_for_filters(days_to_go_back=14)\n",
    "\n",
    "# leave this manual setting here just in case we need to go back to a specific date\n",
    "# events_since_time = datetime.strptime(\"12-01-2024 00:00:00\", \"%m-%d-%Y %H:%M:%S\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events_and_children_since_time() -> CommandCursor:\n",
    "    \"\"\"\n",
    "    Retrieves daily events and associated child information since a specified time.\n",
    "\n",
    "    This function performs an aggregation pipeline on the 'dailyEvents' collection:\n",
    "    1. Matches events with timestamps greater than or equal to 'events_since_time'.\n",
    "    2. Looks up corresponding child information from the 'children' collection.\n",
    "    3. Projects specific fields from both events and child information.\n",
    "\n",
    "    The function also measures and prints the execution time and number of documents returned.\n",
    "\n",
    "    Returns:\n",
    "        CommandCursor: A cursor to iterate over the matching events with child information.\n",
    "    \"\"\"\n",
    "\n",
    "    result = db['dailyEvents'].aggregate([\n",
    "        {\n",
    "            '$match': {\n",
    "                'timestamp': {\n",
    "                    '$gte': events_since_time\n",
    "                }\n",
    "            }\n",
    "        }, {\n",
    "            '$lookup': {\n",
    "                'from': 'children', \n",
    "                'localField': 'childId', \n",
    "                'foreignField': '_id', \n",
    "                'as': 'childInfo'\n",
    "            }\n",
    "        }, {\n",
    "            '$project': {\n",
    "                'notes': 1, \n",
    "                'details': 1, \n",
    "                'eventType': 1, \n",
    "                'childId': 1, \n",
    "                'staffId': 1, \n",
    "                'timestamp': 1, \n",
    "                'childInfo.firstName': 1, \n",
    "                'childInfo.lastName': 1\n",
    "            }\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    # result is just a cursor and doesn't return any data till you iterate over it\n",
    "    print_num_events(result)\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def print_num_events(result: CommandCursor) -> List:\n",
    "    \"\"\"\n",
    "    Prints the execution time and number of documents returned from a CommandCursor.\n",
    "\n",
    "    This function executes the query represented by the CommandCursor, measures the\n",
    "    execution time, and prints both the time taken and the number of documents returned.\n",
    "\n",
    "    Args:\n",
    "        result (CommandCursor): The CommandCursor object representing the query to be executed.\n",
    "\n",
    "    Returns:\n",
    "        List: Returns the data returned by the query.\n",
    "    \"\"\"\n",
    "\n",
    "    # time before we run the query\n",
    "    start_time = time.time()\n",
    "\n",
    "    data = list(result) # this actually runs the query and returns the data\n",
    "    \n",
    "    # time taken to run the query\n",
    "    end_time = time.time()\n",
    "    \n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution time: {execution_time} seconds\")\n",
    "    num_of_docs = len(data)\n",
    "    print(f\"Number of documents: {num_of_docs}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def run_explain_on_pipeline(pipeline, collection_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Runs an explain command on the given pipeline for the specified collection.\n",
    "\n",
    "    This function executes an explain command to analyze the performance of an aggregation\n",
    "    pipeline on a specific collection. It provides detailed execution statistics for the\n",
    "    pipeline, which can be useful for query optimization.\n",
    "\n",
    "    Args:\n",
    "        pipeline (list): The aggregation pipeline to explain.\n",
    "        collection_name (str): The name of the collection to run the explain on.\n",
    "\n",
    "    Returns:\n",
    "       Dict: The result of the explain command.\n",
    "    \"\"\"\n",
    "    \n",
    "    explain_command = {\n",
    "    \"explain\": {\n",
    "        \"aggregate\": collection_name,\n",
    "        \"pipeline\": pipeline,\n",
    "        \"cursor\": {}\n",
    "    },\n",
    "    \"verbosity\": \"allPlansExecution\"\n",
    "}\n",
    "    return db.command(explain_command)\n",
    "    \n",
    "\n",
    "def get_events_since_time(explain: bool = False) -> CommandCursor:\n",
    "    \"\"\"\n",
    "    Retrieves daily events from the database since a specified time.\n",
    "\n",
    "    This function queries the 'dailyEvents' collection for events with timestamps\n",
    "    greater than or equal to the 'events_since_time'. It measures and prints the\n",
    "    execution time and the number of documents returned.\n",
    "\n",
    "    Returns:\n",
    "        CommandCursor: A cursor to iterate over the matching events.\n",
    "    \"\"\"\n",
    "\n",
    "    pipeline = [\n",
    "        {\n",
    "            '$match': {\n",
    "                'timestamp': {\n",
    "                    '$gte': events_since_time\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    if explain:\n",
    "        results = run_explain_on_pipeline(pipeline, \"dailyEvents\")\n",
    "        used_collscan = False\n",
    "        used_index = False\n",
    "        index_name = \"\"\n",
    "        winning_plan = results[\"queryPlanner\"][\"winningPlan\"]\n",
    "        if winning_plan[\"stage\"] == \"COLLSCAN\":\n",
    "            used_collscan = True\n",
    "        if 'inputStage' in winning_plan: # this might not be enough we shall see\n",
    "            if winning_plan['inputStage']['stage'] == \"IXSCAN\":\n",
    "                used_index = True\n",
    "                index_name = winning_plan['inputStage']['indexName']\n",
    "\n",
    "        \n",
    "        parsed_results = {\n",
    "            \"execution_millis\": results[\"executionStats\"][\"executionTimeMillis\"],\n",
    "            \"total_docs_examined\": results[\"executionStats\"][\"totalDocsExamined\"],\n",
    "            \"used_collscan\": used_collscan,\n",
    "            \"used_index\": used_index,\n",
    "            \"index_name\": index_name,\n",
    "            \"total_keys_examined\": results[\"executionStats\"][\"totalKeysExamined\"],\n",
    "            \"total_docs_examined\": results[\"executionStats\"][\"totalDocsExamined\"]\n",
    "        }\n",
    "        \n",
    "        return parsed_results, results\n",
    "    \n",
    "    result = db['dailyEvents'].aggregate(pipeline)\n",
    "\n",
    "    \n",
    "    # result is just a cursor and doesn't return any data till you iterate over it\n",
    "    print_num_events(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limit the number of documents returned\n",
    "\n",
    "Limit the events by a certain time range, example last 7 days.\n",
    "\n",
    "## What problems could we have here?\n",
    "\n",
    "We are not using any index here. If we were to add an index I bet this would be even faster.\n",
    "\n",
    "## Examining the results\n",
    "\n",
    "Running the cell below we will run a query to get all of the events since 7 days ago and get the explain plan for the query.\n",
    "\n",
    "You should get results similar to the following. Notice that we are using a collscan and it is taking an estimated 459ms to run the query. Keep in mind that this is just an estimate and your results might be different.\n",
    "\n",
    "```json\n",
    "{\n",
    "    'execution_millis': 459,\n",
    "    'index_name': '',\n",
    "    'total_docs_examined': 2000000,\n",
    "    'used_collscan': True,\n",
    "    'used_index': False\n",
    " }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'execution_millis': 459,\n",
      " 'index_name': '',\n",
      " 'total_docs_examined': 2000000,\n",
      " 'used_collscan': True,\n",
      " 'used_index': False}\n"
     ]
    }
   ],
   "source": [
    "# By setting explain=True we can get the explain plan for the query. \n",
    "parsed_results, results = get_events_since_time(explain=True)\n",
    "pprint(parsed_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to run the query to see the results.\n",
    "get_events_since_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding an index\n",
    "\n",
    "Anyone who knows SQL knows that creating an index can help speed up queries. \n",
    "\n",
    "What do we add a index to in the above query?\n",
    "When adding an index we should add an index to fields that we are using in a `where clause` or in MongoDB's aggregation pipeline the `$match` or in a `find({'field': 'value'})` on the field. Also we should look at indexes on fields that are used in a `sort`.\n",
    "\n",
    "In the query above we are matching on the timestamp field, let's add an index to that. \n",
    "\n",
    "In MongoDB we can add an index to a field by using the `create_index` method. You also provide a parameter for accending or descending sort order but which do we use? \n",
    "\n",
    "My gut would say we care about the most recent events but there is this note in the docs that stats indexes using descending order can cause performance issues and only use ascending order for indexes. https://www.mongodb.com/docs/manual/core/indexes/create-index/#example so We will start with that, test it, remove it and check descending order and see if there is a difference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'timestamp_1'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add index to the timestamp field in ascending order\n",
    "db['dailyEvents'].create_index([('timestamp', 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the query with the index ascending\n",
    "\n",
    "Run the query in a seperate cell to not have the index creation influence the run time\n",
    "\n",
    "## Getting the explain plan with explain=True\n",
    "First lets run with an explain to make sure we are using the index\n",
    "\n",
    "You should get results similar to the following. Notice that we are using a index and it is taking an estimated 1407 to run the query. Continue on to see why this is.\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "    'execution_millis': 1407,\n",
    "    'index_name': 'timestamp_1',\n",
    "    'total_docs_examined': 990580,\n",
    "    'used_collscan': False,\n",
    "    'used_index': True\n",
    " }\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'execution_millis': 1331,\n",
      " 'index_name': 'timestamp_1',\n",
      " 'total_docs_examined': 990580,\n",
      " 'used_collscan': False,\n",
      " 'used_index': True}\n"
     ]
    }
   ],
   "source": [
    "# First lets clear the query plan cache\n",
    "db.command({\"planCacheClear\": \"dailyEvents\"})\n",
    "# By setting explain=True we can get the explain plan for the query. \n",
    "parsed_results, results = get_events_since_time(explain=True)\n",
    "pprint(parsed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why are the executions times more?\n",
    "\n",
    "We are using an index, why are the execution times more?!?! Notice we are returning 990580 in my example, yours maybe different depending on your dataset. As long as you are returning more then 600_000 or 30% of 2_000_000 documents then you should see the same issue.\n",
    "\n",
    "Why would this happen?\n",
    "\n",
    "When the query would return a large portion of the collection (typically >30% of documents):\n",
    "- The index scan plus document lookup becomes more expensive than a simple collection scan\n",
    "- MongoDB has to look up each document in the index and then fetch the actual document\n",
    "- If you look at the full query plan you will see multiple stages for the query.\n",
    "\n",
    "<\n",
    "## So what does this mean\n",
    "\n",
    "This means that this is why we should always test our queries after adding an index. Just because we added an index doesn't mean that we are actually improving anything.\n",
    "\n",
    "Always follow these steps:\n",
    "- Run the query with an explain. Document how long the query plan stats it will run via the executionTimeMillisEstimate\n",
    "- Calculate if the number of rows that is going to be returned is greater than 30%. If so maybe an index doesn't make sense. \n",
    "- Add the index you plan to use\n",
    "- Run the query explain again. Make sure you document the total executionTimeMillisEstimate not just the executionTimeMillisEstimate for each stage.\n",
    "- If the index makes it worse remove the index. Feel free to try another index just as we are going to down below for descending order just to make sure.\n",
    "- If you have trouble reading the explain plan recreate the query in MongoDB Compass and run the explain there. The visual is much more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_events_since_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a descending index\n",
    "\n",
    "Just for fun, lets add a descending index on the timestamp field to see what happens. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'timestamp_-1'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the index on the timestamp field\n",
    "try:\n",
    "    db['dailyEvents'].drop_index('timestamp_1')\n",
    "except Exception:\n",
    "    print(\"Index does not exist\")\n",
    "# For fun maybe we clear the query plan cache\n",
    "db.command({\"planCacheClear\": \"dailyEvents\"})\n",
    "\n",
    "# Recreate the index on the timestamp field in descending order\n",
    "db['dailyEvents'].create_index([('timestamp', -1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets clear the query plan cache\n",
    "db.command({\"planCacheClear\": \"dailyEvents\"})\n",
    "# By setting explain=True we can get the explain plan for the query. \n",
    "parsed_results, results = get_events_since_time(explain=True)\n",
    "pprint(parsed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing the descending index\n",
    "You shouldn't see any difference here, maybe a couple of milliseconds faster but still way slower than not using an index so lets remove it and carry on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the descending order index\n",
    "try:\n",
    "    db['dailyEvents'].drop_index('timestamp_-1')\n",
    "except Exception:\n",
    "    print(\"Index does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What bout on a smaller window of time\n",
    "\n",
    "Let's see if the index actually makes a difference on this dataset. We will do this by running the query with a $gte with looking for 3 days of data. \n",
    "\n",
    "First lets create the ascending index, then run the query and explain for 3 days of data.\n",
    "\n",
    "I am getting results back similar to the following:\n",
    "\n",
    "```json\n",
    "{\n",
    "    'execution_millis': 388,\n",
    "    'index_name': 'timestamp_1',\n",
    "    'total_docs_examined': 256847,\n",
    "    'used_collscan': False,\n",
    "    'used_index': True\n",
    " }\n",
    "```\n",
    "\n",
    "Without and index we are using a collscan and it is taking an estimated 459ms to run the query. This is 388 to 400 with the index. In my opnion it may not be worth it with this level of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'timestamp_1'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the index on the timestamp field in ascending order\n",
    "db['dailyEvents'].create_index([('timestamp', 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the time we are querying from\n",
    "events_since_time = get_time_for_filters(days_to_go_back=3)\n",
    "\n",
    "# By setting explain=True we can get the explain plan for the query. \n",
    "parsed_results, results = get_events_since_time(explain=True)\n",
    "pprint(parsed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the index on the timestamp field\n",
    "try:\n",
    "    db['dailyEvents'].drop_index('timestamp_1')\n",
    "except Exception:\n",
    "    print(\"Index does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's explode our dataset\n",
    "\n",
    "This is a small dataset so lets see if we can get a feel for how MongoDB works. We will create a new dataset with the same amount of children and staff members but greatly increase the number of events. We will keep the same number of events within the last 30 days but increase the number of older events overall.\n",
    "\n",
    "First we need to drop the collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the children, staff, and dailyEvents collections\n",
    "db['children'].drop()\n",
    "db['staff'].drop()\n",
    "db['dailyEvents'].drop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to generate the new dataset. Open your .env file and set GENERATE_OLD_EVENTS=true\n",
    "\n",
    "Then open your terminal to the root of this repo and run the following command: `uv run generate_data.py`\n",
    "\n",
    "You should see an output similar to the following:\n",
    "```bash\n",
    "Inserting Children Data\n",
    "Inserting Staff Data\n",
    "Inserting Events Data\n",
    "Generating old events\n",
    "```\n",
    "\n",
    "Then after a while you will see it start inserting the data. This could take a while as it is a lot of data it is generating.\n",
    "\n",
    "Once this is done you should have the same database setup you had before but with\n",
    "\n",
    "- 10k children\n",
    "- 100 staff members\n",
    "- 4,000,000 events. 2 million of which are in the last 30 days and 2 million of which are older. Lets see how these queries perform with this new dataset.\n",
    "\n",
    "First lets make sure we have a clean slate of indexes\n",
    "\n",
    "Then lets query for 14 days worth of data without an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing indexes (excluding _id_):\n",
      "Name: timestamp_1\n"
     ]
    }
   ],
   "source": [
    "# List all indexes in the dailyEvents collection except for _id_\n",
    "indexes = db['dailyEvents'].list_indexes()\n",
    "non_id_indexes = [index for index in indexes if index['name'] != '_id_']\n",
    "print(\"Existing indexes (excluding _id_):\")\n",
    "for index in non_id_indexes:\n",
    "    print(f\"Name: {index['name']}\")\n",
    "    db['dailyEvents'].drop_index(index['name'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the query without an index\n",
    "\n",
    "The results are a little more interesting this time but well within what we would expect. We are using a collscan and it is taking an estimated 897ms to run the query over 4_000_000 documents.\n",
    "\n",
    "```json\n",
    "{\n",
    "    'execution_millis': 897,\n",
    "    'index_name': '',\n",
    "    'total_docs_examined': 4000000,\n",
    "    'used_collscan': True,\n",
    "    'used_index': False\n",
    " }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for 14 days with of data\n",
    "\n",
    "# reset the time we are querying from\n",
    "events_since_time = get_time_for_filters(days_to_go_back=14)\n",
    "\n",
    "# By setting explain=True we can get the explain plan for the query. \n",
    "parsed_results, results = get_events_since_time(explain=True)\n",
    "pprint(parsed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readd and query with an index\n",
    "\n",
    "Lets add the index on the timestamp field in ascending order then requery for 14 days worth of data and see if we see any difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'timestamp_1'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recreate the index on the timestamp field in ascending order\n",
    "db['dailyEvents'].create_index([('timestamp', 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see results similar to the following. Notice that we are using a index and it is taking an estimated 1469ms to run the query. You can also see the total number of documents that were scanned and the total number of keys that were scanned are the same. This means that we are getting every document from the index.\n",
    "\n",
    "```json\n",
    "{\n",
    "    'execution_millis': 1469,\n",
    "    'index_name': 'timestamp_1',\n",
    "    'total_docs_examined': 994169,\n",
    "    'total_keys_examined': 994169,\n",
    "    'used_collscan': False,\n",
    "    'used_index': True\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requery for 14 days with of data\n",
    "\n",
    "# reset the time we are querying from\n",
    "events_since_time = get_time_for_filters(days_to_go_back=14)\n",
    "\n",
    "# By setting explain=True we can get the explain plan for the query. \n",
    "parsed_results, results = get_events_since_time(explain=True)\n",
    "pprint(parsed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So this is neat and all but....\n",
    "\n",
    "There has to be a better way to do this. Odd's are you want just the events for a specific child or staff memeber so you could create compound indexes on the date and the id's. However, let's look at a few more options.\n",
    "\n",
    "## Where did we go wrong?\n",
    "\n",
    "Odds are we jumped into designing the database without thinking about how we would query for data. Classic rookie mistake with a document database vs just doing what we do with a relational database. When we design for a document database we should design for how we plan to use the data.\n",
    "\n",
    "### Embeded Data\n",
    "\n",
    "Let's think about how our users will be accessing the data. Our customers are daycare centers but their customers are parents of children. They want to be able to see what their children have done in the daycare that day. Lets ask ourselves a series of questions:\n",
    "- Do we always need this data when we query for a child?\n",
    "- What's the date range we need to query for?\n",
    "- Is the data unbounded?\n",
    "- Can we store some embeded but also have it stored in a separate collection for other access?\n",
    "\n",
    "\n",
    "For this example let's say parent's want to be able to open the app and see everything their child has done today and they want it to be as fast as possible, however, if they want to run reports and see data for other days they are ok with a small load time. So lets keep our daily events collection as simple as possible. This is known as the subset pattern.\n",
    "\n",
    "Let's also update our children collection to include a field for the clients location they are at. This is an array of locations that has a start_date and and end_date.\n",
    "\n",
    "This will allow us to easily query for their current location and previous locations of when they were there.\n",
    "\n",
    "So let's redesign our children collection:\n",
    "\n",
    "```\n",
    "{\n",
    "    ObjectId _id\n",
    "    string firstName\n",
    "    string lastName\n",
    "    date dateOfBirth\n",
    "    datetime enrollmentDate\n",
    "    array customerLocation [{\n",
    "        ObjectId _id\n",
    "        string name\n",
    "    }]\n",
    "    array allergies ['nuts', 'dairy', 'gluten', 'soy', 'eggs', 'fish']\n",
    "    object emergencyContact {\n",
    "        string name\n",
    "        string phone\n",
    "        string relationship\n",
    "    }\n",
    "    array dailyEvents [{\n",
    "        datetime timestamp\n",
    "        string eventType\n",
    "        string details\n",
    "        string notes\n",
    "    }]\n",
    "}\n",
    "```\n",
    "\n",
    "Adding a customer location field also means we probably need to have a location collection to store all of the locations and their information.\n",
    "\n",
    "```\n",
    "locations{\n",
    "    ObjectId _id\n",
    "    string name\n",
    "    string address\n",
    "    string city\n",
    "    string state\n",
    "    string zip\n",
    "    string country\n",
    "    contactInfo {\n",
    "        string phone\n",
    "        string email\n",
    "    }\n",
    "    string website\n",
    "}\n",
    "```\n",
    "\n",
    "#### Handling old events\n",
    "\n",
    "So what do we do with old events. Do we need to keep them? Yes we do as parents want to be able to go back and see what their child did in the daycare on previous days.\n",
    "\n",
    "To handle this we will keep our dailyEvents collection. Every day we will run a cron to move events for each child and insert them into the dailyEvents collection. This way we can keep track of all of the events for each child.\n",
    "\n",
    "TBD: Update the dailyEvents collection. We should follow the bucket pattern for dailyEvents per location. Inside each location we should have a bucket for each day. This way we can keep track of all of the events for each child. This will also allow us to index the data easier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
