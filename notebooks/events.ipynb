{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MongoDB Childcare Database\n",
    "This databse is a collection of data about children, staff and their events in a daycare.\n",
    "\n",
    "You will notice the first way we have it setup is very relational. We have a table for children, a table for staff and a table for events. The events table has a reference to the children and staff tables. This is how you would see this setup in a relational database. \n",
    "\n",
    "# Get the daily events and child info for each event in aggregation pipeline\n",
    "\n",
    "## Why is this bad\n",
    "\n",
    "In the query below we are joining the child info with the daily events table. We are also getting every single event. Odds are we don't need every single event. \n",
    "\n",
    "**Stats:**\n",
    "- This returns 2000000 documents(2 million)\n",
    "- This takes roughly 23 seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, tzinfo, timezone\n",
    "import time\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from pymongo.synchronous.command_cursor import CommandCursor\n",
    "\n",
    "client = MongoClient('mongodb://root:password@localhost:27017/')\n",
    "\n",
    "db = client['daycare_db']\n",
    "\n",
    "# used in queries to get all events since a certain date\n",
    "events_since_time = datetime.strptime(\"12-08-2024 00:00:00\", \"%m-%d-%Y %H:%M:%S\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = client['daycare_db']['dailyEvents'].aggregate([\n",
    "    {\n",
    "        '$lookup': {\n",
    "            'from': 'children', \n",
    "            'localField': 'childId', \n",
    "            'foreignField': '_id', \n",
    "            'as': 'child_info'\n",
    "        }\n",
    "    }, {\n",
    "        '$unwind': '$child_info'\n",
    "    }, {\n",
    "        '$project': {\n",
    "            'eventType': 1, \n",
    "            'childId': 1, \n",
    "            'timestamp': 1, \n",
    "            'details': 1, \n",
    "            'firstName': '$child_info.firstName', \n",
    "            'lastName': '$child_info.lastName'\n",
    "        }\n",
    "    }\n",
    "])\n",
    "\n",
    "data = list(result)\n",
    "num_of_docs = len(data)\n",
    "print(f\"Number of documents: {num_of_docs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events_and_children_since_time() -> CommandCursor:\n",
    "    \"\"\"\n",
    "    Retrieves daily events and associated child information since a specified time.\n",
    "\n",
    "    This function performs an aggregation pipeline on the 'dailyEvents' collection:\n",
    "    1. Matches events with timestamps greater than or equal to 'events_since_time'.\n",
    "    2. Looks up corresponding child information from the 'children' collection.\n",
    "    3. Projects specific fields from both events and child information.\n",
    "\n",
    "    The function also measures and prints the execution time and number of documents returned.\n",
    "\n",
    "    Returns:\n",
    "        CommandCursor: A cursor to iterate over the matching events with child information.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    result = db['dailyEvents'].aggregate([\n",
    "        {\n",
    "            '$match': {\n",
    "                'timestamp': {\n",
    "                    '$gte': events_since_time\n",
    "                }\n",
    "            }\n",
    "        }, {\n",
    "            '$lookup': {\n",
    "                'from': 'children', \n",
    "                'localField': 'childId', \n",
    "                'foreignField': '_id', \n",
    "                'as': 'childInfo'\n",
    "            }\n",
    "        }, {\n",
    "            '$project': {\n",
    "                'notes': 1, \n",
    "                'details': 1, \n",
    "                'eventType': 1, \n",
    "                'childId': 1, \n",
    "                'staffId': 1, \n",
    "                'timestamp': 1, \n",
    "                'childInfo.firstName': 1, \n",
    "                'childInfo.lastName': 1\n",
    "            }\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    # result is just a cursor and doesn't return any data till you iterate over it\n",
    "    print_num_events(result)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution time: {execution_time} seconds\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def print_num_events(result: CommandCursor):\n",
    "    data = list(result) # this actually runs the query and returns the data\n",
    "    num_of_docs = len(data)\n",
    "    print(f\"Number of documents: {num_of_docs}\")\n",
    "\n",
    "\n",
    "def run_explain_on_pipeline(pipeline, collection_name: str):\n",
    "    explain_command = {\n",
    "    \"explain\": {\n",
    "        \"aggregate\": collection_name,\n",
    "        \"pipeline\": pipeline,\n",
    "        \"cursor\": {}\n",
    "    },\n",
    "    \"verbosity\": \"executionStats\"\n",
    "}\n",
    "    explain_results = db.command(explain_command)\n",
    "    \n",
    "    pprint(explain_results)\n",
    "\n",
    "def get_events_since_time(explain: bool = False) -> CommandCursor:\n",
    "    \"\"\"\n",
    "    Retrieves daily events from the database since a specified time.\n",
    "\n",
    "    This function queries the 'dailyEvents' collection for events with timestamps\n",
    "    greater than or equal to the 'events_since_time'. It measures and prints the\n",
    "    execution time and the number of documents returned.\n",
    "\n",
    "    Returns:\n",
    "        CommandCursor: A cursor to iterate over the matching events.\n",
    "    \"\"\"\n",
    "    \n",
    "    pipeline = [\n",
    "        {\n",
    "            '$match': {\n",
    "                'timestamp': {\n",
    "                    '$gte': events_since_time\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    if explain:\n",
    "        run_explain_on_pipeline(pipeline, \"dailyEvents\")\n",
    "        return\n",
    "    \n",
    "    result = db['dailyEvents'].aggregate(pipeline)\n",
    "\n",
    "    \n",
    "    # result is just a cursor and doesn't return any data till you iterate over it\n",
    "    print_num_events(result)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution time: {execution_time} seconds\")\n",
    "\n",
    "    # return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limit the number of documents returned\n",
    "\n",
    "Limit the events by a certain time range, example last 3 days. This dataset was created on 12-11-2024 so we to get the last 3 days we would use 12-08-2024.\n",
    "\n",
    "## Why is this bad\n",
    "\n",
    "We are not using any index here. If we were to add an index I bet this would be even faster.\n",
    "\n",
    "**Stats:**\n",
    "- This returns 714306 documents\n",
    "- Uses a collscan\n",
    "- Has an 'executionTimeMillisEstimate': 486"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By setting explain=True we can get the explain plan for the query. \n",
    "get_events_since_time(explain=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 714306\n",
      "Execution time: 3.2660820484161377 seconds\n"
     ]
    }
   ],
   "source": [
    "# Now let run the query without explain to see how long it takes\n",
    "get_events_since_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding an index\n",
    "\n",
    "Anyone who knows SQL knows that creating an index can help speed up queries. What do we add a index to in the above query?\n",
    "When adding an index we should add an index to fields that we are using in a where clause or in MongoDB's aggregation pipeline the $match or anything we are sorting on.\n",
    "\n",
    "In the query above we are matching on the timestamp field. We should add an index to the timestamp field. In MongoDB we can add an index to a field by using the create_index method. You also provide a parameter for accending or descending but which do we use? My gut would say we care about the most recent events but there is this note in the docs that stats indexes using descending order can cause performance issues and only use ascending order for indexes. https://www.mongodb.com/docs/manual/core/indexes/create-index/#example so We will start with that, test it, remove it and check descending order and see if there is a difference.\n",
    "\n",
    "\n",
    "**Stats:**\n",
    "With ascending order index on timestamp\n",
    "- This returns 714306 documents\n",
    "- This takes roughly  seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'timestamp_1'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add index to the timestamp field in ascending order\n",
    "db['dailyEvents'].create_index([('timestamp', 1)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the query with the index ascending\n",
    "\n",
    "Run the query in a seperate cell to not have the index creation influence the run time\n",
    "\n",
    "## Getting the explain plan with explain=True\n",
    "First lets run with an explain to make sure we are using the index\n",
    "\n",
    "This output should show you we have an index 'indexName': 'timestamp_1'. So we are infact using the index.\n",
    "\n",
    "## Wait its slower using the index\n",
    "Looking at the explain plan we see that the query now has a **'executionTimeMillisEstimate': 1018**! What happened?\n",
    "\n",
    "When the query would return a large portion of the collection (typically >30% of documents):\n",
    "- The index scan plus document lookup becomes more expensive than a simple collection scan\n",
    "- MongoDB has to look up each document in the index and then fetch the actual document\n",
    "- You can see the two stages in the image below from Compass\n",
    "\n",
    "<\n",
    "## So what does this mean\n",
    "\n",
    "This means that this is why we should always test our queries after adding an index. Just because we added an index doesn't mean that we are actually improving anything.\n",
    "\n",
    "Always follow these steps:\n",
    "- Run the query with an explain. Document how long the query plan stats it will run via the executionTimeMillisEstimate\n",
    "- Calculate if the number of rows that is going to be returned is greater than 30%. If so maybe an index doesn't make sense. \n",
    "- Add the index you plan to use\n",
    "- Run the query explain again. Make sure you document the total executionTimeMillisEstimate not just the executionTimeMillisEstimate for each stage.\n",
    "- If the index makes it worse remove the index. Feel free to try another index just as we are going to down below for descending order just to make sure.\n",
    "- If you have trouble reading the explain plan recreate the query in MongoDB Compass and run the explain there. The visual is much more readable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "get_events_since_time(explain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_events_since_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a descending index\n",
    "\n",
    "Just for fun, lets add a descending index on the timestamp field to see what happens. \n",
    "This actually ran in ~ 900ms which is faster then the ascending index, which goes against the docs?!?!\n",
    "\n",
    "However it is still slower then no index. So we will remove this index as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'timestamp_-1'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the index on the timestamp field\n",
    "try:\n",
    "    db['dailyEvents'].drop_index('timestamp_1')\n",
    "except Exception:\n",
    "    print(\"Index does not exist\")\n",
    "# For fun maybe we clear the query plan cache\n",
    "db.command({\"planCacheClear\": \"dailyEvents\"})\n",
    "\n",
    "# Recreate the index on the timestamp field in descending order\n",
    "db['dailyEvents'].create_index([('timestamp', -1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_events_since_time(explain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the descending order index\n",
    "try:\n",
    "    db['dailyEvents'].drop_index('timestamp_-1')\n",
    "except Exception:\n",
    "    print(\"Index does not exist\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
